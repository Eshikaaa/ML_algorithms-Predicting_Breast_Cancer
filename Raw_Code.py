# -*- coding: utf-8 -*-
"""Cancer Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXs2c5s90gmYYVicXG0f9QWprF3BPKMq
"""

import numpy
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df=pd.read_csv("data.csv")

df.head()

df.info()

df.isna().sum()

df.shape

df= df.dropna(axis=1)

df.shape

df.describe()

#get cnt of malignant(M) and benign(B) cells
# A benign tumor has distinct, smooth, regular borders. A malignant tumor has irregular borders and grows faster than a
# benign tumor. A malignant tumor can also spread to other parts of your body. A benign tumor can become quite large,
# but it will not invade nearby tissue or spread to other parts of your body.

df['diagnosis'].value_counts()

sns.countplot(x='diagnosis', data=df, label="count")
plt.title('Count of Benign and Malignant Diagnoses')
plt.xlabel('Diagnosis')
plt.ylabel('Count')
plt.show()

#label encoding (convert value of M,B into 1 & 0)
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
df.iloc[:,1]= labelencoder_Y.fit_transform(df.iloc[:,1].values)

df.head()

#showing relation between pairs
sns.pairplot(df.iloc[:,1:5], hue="diagnosis")

#get the correlation
df.iloc[:,1:32].corr()

#visualize the correlation:heatmap
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:10].corr(), annot=True, fmt=".0%")

#split dataset into independent(X) & dependent(Y) datasets
X=df.iloc[:,2:31].values
Y=df.iloc[:,1].values

#splitting data into training & testing datasets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.2, random_state=0)

#feature scaling, no outliers should be present
from sklearn.preprocessing import StandardScaler
X_train= StandardScaler().fit_transform(X_train)
X_test= StandardScaler().fit_transform(X_test)

def eshi(X_train, Y_train):
    # Import necessary libraries
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import LabelEncoder

    # Initialize models
    log = LogisticRegression(random_state=0)
    tree = DecisionTreeClassifier(random_state=0, criterion="entropy")
    forest = RandomForestClassifier(random_state=0, criterion="entropy", n_estimators=10)

    # Encode the labels in Y_train
    le = LabelEncoder()
    Y_train_encoded = le.fit_transform(Y_train)

    # Fit the models
    log.fit(X_train, Y_train_encoded)
    tree.fit(X_train, Y_train_encoded)
    forest.fit(X_train, Y_train_encoded)

    # Print training accuracy
    print('[0] Logistic Regression accuracy', log.score(X_train, Y_train_encoded))
    print('[1] Decision Tree accuracy', tree.score(X_train, Y_train_encoded))
    print('[2] Random Forest accuracy', forest.score(X_train, Y_train_encoded))

    return log, tree, forest, le

# Train the models
log, tree, forest, le = eshi(X_train, Y_train)

# Encode the Y_test labels
Y_test_encoded = le.transform(Y_test)

# Import necessary metrics
from sklearn.metrics import accuracy_score, classification_report

# Evaluate the models
models = [log, tree, forest]
for i, model in enumerate(models):
    print("Model", i)
    predictions = model.predict(X_test)
    print(classification_report(Y_test_encoded, predictions))
    print('Accuracy : ', accuracy_score(Y_test_encoded, predictions))

#prediction of random forest
pred= model[2].predict(X_test)
print('Predicted val')
print(pred)
print('actual val')
print(Y_test)

from joblib import dump
dump(model[2],"Cancer_prediction_model.joblib")